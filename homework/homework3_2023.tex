\documentclass[11pt, a4paper]{article}
%\usepackage{geometry}
\usepackage[inner=1.5cm,outer=1.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\pagestyle{empty}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage[usenames,dvipsnames]{color}
\definecolor{darkblue}{rgb}{0,0,.6}
\definecolor{darkred}{rgb}{.7,0,0}
\definecolor{darkgreen}{rgb}{0,.6,0}
\definecolor{red}{rgb}{.98,0,0}
\usepackage[colorlinks,pagebackref,pdfusetitle,urlcolor=darkblue,citecolor=darkblue,linkcolor=darkred,bookmarksnumbered,plainpages=false]{hyperref}
\renewcommand{\thefootnote}{\fnsymbol{footnote}}



\thispagestyle{plain}

%%%%%%%%%%%% LISTING %%%
\usepackage{listings}
\usepackage{caption}
\DeclareCaptionFont{white}{\color{white}}
\DeclareCaptionFormat{listing}{\colorbox{gray}{\parbox{\textwidth}{#1#2#3}}}
\captionsetup[lstlisting]{format=listing,labelfont=white,textfont=white}
\usepackage{verbatim} % used to display code
\usepackage{fancyvrb}
\usepackage{acronym}
\usepackage{amsthm}
\VerbatimFootnotes % Required, otherwise verbatim does not work in footnotes!



\definecolor{OliveGreen}{cmyk}{0.64,0,0.95,0.40}
\definecolor{CadetBlue}{cmyk}{0.62,0.57,0.23,0}
\definecolor{lightlightgray}{gray}{0.93}



\lstset{
%language=bash,                          % Code langugage
basicstyle=\ttfamily,                   % Code font, Examples: \footnotesize, \ttfamily
keywordstyle=\color{OliveGreen},        % Keywords font ('*' = uppercase)
commentstyle=\color{gray},              % Comments font
numbers=left,                           % Line nums position
numberstyle=\tiny,                      % Line-numbers fonts
stepnumber=1,                           % Step between two line-numbers
numbersep=5pt,                          % How far are line-numbers from code
backgroundcolor=\color{lightlightgray}, % Choose background color
frame=none,                             % A frame around the code
tabsize=2,                              % Default tab size
captionpos=t,                           % Caption-position = bottom
breaklines=true,                        % Automatic line breaking?
breakatwhitespace=false,                % Automatic breaks only at whitespace?
showspaces=false,                       % Dont make spaces visible
showtabs=false,                         % Dont make tabls visible
columns=flexible,                       % Column format
morekeywords={__global__, __device__},  % CUDA specific keywords
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\begin{center}
  {\Large \textsc{Problem Set 3}}

  MGMT 737
\end{center}


\begin{enumerate}
\item \textbf{Quantile Regression}. This analysis will use the dataset
  from Problem Set 1, \texttt{lalonde\_nsw.csv} (which I will refer to
  as NSW), as well as the dataset from Problem Set 2,
  \texttt{lalonde\_psid.csv} (which I will call PSID).
  \begin{enumerate}
  \item We will begin by defining an estimation approach for doing
    quantile regression that doesn't require linear programming. This
    approach comes from Gary Chamberlain (in Chamberlain (1994), and
    discussed in Angrist et al. (2006)). 

    Let $X$ be a (discrete) right hand side variable with $J$ discrete
    values.  For each $j$ value of $X = x_{j}$, calculate
    $\hat{\pi}_{\tau}(x) = Q_{\tau}(Y|X_{j})$, which is the $\tau$
    percentile of the outcome variable, conditional on the value of
    $X$, and $\hat{p}_{j}$, which is the empirical probability of
    $X = x_{j}$.  Do so using the PSID dataset for
    $X = \textrm{education}$, for $\tau = (0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)$
  \item Given these inputs, the quantile regression slope
    estimates is just
    $$ \hat{\beta}_{\tau} = \arg\min_{b} \sum_{j} (\hat{\pi}_{\tau}(x_{j}) - x_{j}b)^{2}\hat{p}_{j}.$$
    This is a simple (weighted) linear regression (or minimum distance
    problem), with the diagonal weight matrix
    $\hat{W} = diag(\hat{p}_{1}, \ldots, \hat{p}_{J})$. Estimate
    $\hat{\beta}_{\tau}$ for the education example above.
  \item Our variance estimator is the sum of two terms (coming from
    uncertainty in the QCF, and the estimation of the slope
    conditional on those terms), $V$ and $D$:
    \begin{align*}
  V &= \left(\boldsymbol{x}'\hat{W}\boldsymbol{x}\right)^{-1}\boldsymbol{x}'\hat{W}\Sigma\hat{W}\boldsymbol{x}\left(\boldsymbol{x}'\hat{W}\boldsymbol{x}\right)^{-1},\\
  \Sigma &= diag(\sigma^{2}_{\tau,1}/p_{1}, \ldots, \sigma^{2}_{\tau,J}/p_{J})\\
  D &= \left(\boldsymbol{x}'\hat{W}\boldsymbol{x}\right)^{-1}\boldsymbol{x}'\hat{W}\Delta\hat{W}\boldsymbol{x}\left(\boldsymbol{x}'\hat{W}\boldsymbol{x}\right)^{-1},\\
  \Delta &= diag((\pi_{\tau,1} - x_{1}\beta_{\tau})^{2}/p_{1}, \ldots, (\pi_{\tau,J} - x_{J}\beta_{\tau})^{2}/p_{J}).
\end{align*}
Everything here should be straight forward to estimate, except for
$\sigma^{2}_{\tau,j}$. To do this, define the following order
statistics:
\begin{align*}
  b_{j} &= \max \left\{1, \mathrm{round}\left(\tau N_{j} - z_{1-\alpha/2}\sqrt{\tau(1-\tau)N_{j}}\right)\right\}\\
  t_{j} &= \min \left\{N_{j}, \mathrm{round}\left(\tau N_{j} + z_{1-\alpha/2}\sqrt{\tau(1-\tau)N_{j}}\right)\right\},
\end{align*}
where $\mathrm{round}(a)$ rounds to the closest integer, and
$z_{1-\alpha/2} = 1.96$, typically, and $N_{j}$ is the number of
observations in the $j$th bin of $X$. Then,
\begin{equation}
  \hat{\sigma}^{2}_{\tau,j} = N_{j}\left(\frac{y_{j(t_{j})}-y_{j(b_{j})}}{2 z_{1-\alpha}}\right)^{2}.
\end{equation}
Report the standard error on your estimates, which is calculated as $\sqrt{(V+D)/N}$
\item Finally, using the NSW dataset, calculate the
  $\tau = (0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9)$ treatment effects, and their
  standard errors.
\end{enumerate}
\item \textbf{Poisson Regression}. This analysis will use the dataset
  \texttt{Detroit.csv}. You should use built-in pacakges for this --
  in \texttt{R}, use the \texttt{fixest} library. In \texttt{Stata},
  use the \texttt{reghdfe} and \texttt{ppmlhdfe} packages.
  \begin{enumerate}
  \item Run an OLS regression of \texttt{flows} (the number of workers
    who work in \texttt{home\_ID} and work in \texttt{work\_ID}) on
    \texttt{distance\_Google\_miles}, and include \texttt{home\_ID} and
    \texttt{work\_ID} fixed effects (absorb them), and cluster on
    \texttt{home\_ID}. Report the coefficient and standard error on
    \texttt{distance\_Google\_miles}.
  \item Run an OLS regression of \texttt{log(flows)} on
    \texttt{log(distance\_Google\_miles)} and include \texttt{home\_ID}
    and \texttt{work\_ID} fixed effects , omitting the cells with zero
    flows. Cluster on \texttt{home\_ID}. Report the coefficient and
    standard error on \texttt{log(distance\_Google\_miles)}.
  \item Repeat part 1b, but instead of omitting the zero cells, run
    the OLS regression of \texttt{log(c + flows)} for c = 0.1, 1 and
    10. Compare how your coefficients change. 
  \item Finally, repeat part 1a using Poisson regression, and contrast
    the estimates to Part b and c.
  \end{enumerate}
\item \textbf{Duration Modeling}. This analysis will use the dataset
  \texttt{acs\_duration.csv}. The
  \texttt{acs\_duration.csv} dataset is from the American Community
  Survey in 2019, and has heads of households' responses to the
  question ``How long have you lived in this home?''
  (\texttt{moving\_approx}  -- in reality, this value is given as
    a range in the public data -- I have imputed using the midpoint. A
    fun exercise left to the reader is to think about how to
    generalize this problem using ranges.) and homeownership
  (\texttt{homeowner} vs. \texttt{renter}).
    \begin{enumerate}
    \item Using the ACS data,write down how to estimate the unconditional probability that a household stays in a home for T or more years, using the available data. Estimate this for $T=7$ and report the value.
    \item Calculate the hazard rate for each observed value of
      \texttt{moving\_approx}. Report this value for $T=7$.
    \item Recalculate these hazard values for $T=7$ for homeowners and
      renters. Contrast the difference in hazard rates over time.
    \end{enumerate}
    % Next year, add a problem on shrinkage
%\item \textbf{Shrinkage}
%  \begin{itemize}
%  \item 
%  \end{itemize} 
   
\end{enumerate}


\end{document}